# Using NEAT to Create AI Racecars
Check out my YouTube video explaining this topic!
<br/>https://www.youtube.com/watch?v=pHWA1HfEwL8

## Implementation of the AI


## Background on Neural Neworks and NeuroEvolution of Augmenting Topologies
Written by Krushay Bhavsar, April 12th 2021

#### The Structure of a Simple Neural Network

<p>Used widely in the area of machine learning, neural networks (NN) are collections of neurons, or nodes, that are interconnected with several connections and are organized in layers. The purpose of NNs, simply put, is to take in data, perform a series of calculations, and recognize a relationship. They have many applications, ranging from analyzing natural language text to predicting stock prices. There are various structures a NN can be laid out in, some better at performing their task than others. A potential form of a simple NN is depicted in the diagram below:</p>
<p align="center"><img src="https://user-images.githubusercontent.com/68528325/121792343-97276480-cbc1-11eb-9610-a577d766c8fb.png" alt="Neural Network Diagram" width="400"/></p>
<p>The leftmost layer on the diagram is known as the input layer. Input layers are the series of nodes that take in the data given to them. The rightmost layer is the output layer, which takes the data from the previous layer to determine the final result. The result of the output layer can mean various things depending on the goal of the NN. Layers between the input and the output layer are known as hidden layers, which is the place where most of the computation is done. In the simplest form of a NN, each node has a connection to each of the nodes in the next layer. For simplicity’s sake, a node can be thought of as a place that holds a number for now. Each connection in the NN also contains a number, called a weight. Because of this structure, the information processing mechanism can be thought of as a system in which activations from one layer bring about activations in the next layer. In other words, data from the input layer travels down into the next layer, causing certain neurons to fire in that layer, which then causes other neurons to fire in the following layer. This continues until it has reached the output layer, which uses the previous layer’s data to indicate an output.</p>

#### Calculating Nodes

<p>To answer the question of how exactly the neural network knows which neurons to fire, focus on a single neuron. Below is a diagram depicting the process a neuron undergoes to calculate its value:</p>
<p align="center"><img src="https://user-images.githubusercontent.com/68528325/121792411-6267dd00-cbc2-11eb-9e60-fe5dd954baf4.png" alt="Calculating a Node Diagram" width="400"/></p>
<p>A NN takes the numerical value of each previous node that connects to the node it is attempting to calculate and multiples that number by the weight of the associated connection. It then repeats the process for each of the nodes and connections in the previous layer and takes the sum of all those numbers. The mathematical expression <img src="https://render.githubusercontent.com/render/math?math=\sum_{j=1}^{n} x_{j} w_{j}"> be used to express this, where n represents the number of neurons in the previous layer, x represents the value of a node in the previous layer, and w represents the value of the weight of the connection that connects from node x to the node we are calculating the value of. After calculating this number, it is plugged into something known as an activation function. Several activation functions can be used, some more fit than others for their purpose. Some of the more used ones are sigmoid, rectified linear unit (ReLU), and tanh. Activation functions essentially “squish” the number given by <img src="https://render.githubusercontent.com/render/math?math=\sum_{j=1}^{n} x_{j} w_{j}"> into a smaller range. The figure below displays a few activation functions and the range they “squish” the numbers into:</p> 
<p align="center"><img src="https://user-images.githubusercontent.com/68528325/121792606-4ca7e700-cbc5-11eb-9d6d-d64134a4402a.png" alt="Types of Activation Functions" width="400"/></p>
<p>The output of these functions becomes the value of the node. This entire process repeats to calculate the value of each node until a value is calculated for each node in the output layer. Of course, this is an extremely oversimplified way of putting this. In almost all applicable neural networks, programmers may need to use different types of layers or may need to include bias values and their calculations. Despite everything that has been explained so far, the “artificial intelligence” aspect of neural networks has not been touched.</p>

#### The Learning Process Used by Ordinary Neural Networks

<p>Plain neural networks may learn through processes known as backpropagation and gradient descent. The processes essentially tweak the weights and biases of the neural network to improve its accuracy and reduce the value given by something known as a loss function. A summary of how this works is that a neural network is initially given labeled data for it to train with—similar to giving a student a practice worksheet with the answer key on the back. The neural network takes its guess on the first “question” and then looks at the “answer key”. It uses a loss function to recognize how off it was from the correct answer and tweaks its weights accordingly. It then moves on to the next problem and repeats the process until it has reached a certain accuracy. Once a neural network has “practiced” with all the training data, it can be referred to as a trained neural network. The trained neural network can then be given unlabelled data and can accurately predict the outcome of the data. Although this is how most ordinary NNs learn, genetic algorithms take a slightly different approach.</p>

#### Genetic Algorithms

<p>Genetic Algorithms are algorithms that take advantage of the process of evolution and mimic natural selection. Take for example a simple game of Flappy Bird. If a programmer wanted to create an AI that could play the game of Flappy Bird and over some time get better, they could use genetic algorithms to generate species of birds, each with their own neural network. Depending on the bird that performs the best, the genetic algorithm would breed the birds to form a new generation of better-performing birds. The consistently poor-performing variants of the bird would eventually go extinct, while the better birds would thrive. This concept is used in many genetic algorithms and is a fundamental part of neuroevolution. To understand exactly how the evolution process occurs and how it helps create a better performing generation, one can focus on understanding the more recent and popular genetic algorithm—NeuroEvolution of Augmenting Topologies (NEAT).</p>

#### The Evolution Process Used by Genetic Algorithms Like NEAT

<p>NEAT is a slight variation of normal genetic algorithms. Initially, each member of the population starts with the simplest form of the neural network. As training progresses, NEAT changes the topology of the neural networks based on the performance of the members of the population. One thing different about the NNs used in NEAT is that they do not have layers. Instead, nodes are added to various locations on the connections of other nodes. Below is a diagram of what the genotype and phenotype of a specific individual may look like:</p>
<p align="center"><img src="https://user-images.githubusercontent.com/68528325/121792665-e40d3a00-cbc5-11eb-8169-e715243a5446.png" alt="Genotype and Phenotype Diagram" width="400"/></p>
<p>Each genome has two sets of genes: node genes and connection genes. Each connection has an innovation number, which is a number that indicates when the connection has been created. This innovation number is a fundamental reason why NEAT is so efficient at crossovers. The interesting part about innovation numbers is that they are shared across all the NNs present in the simulation. Whenever a new connection that has never been created has been made, a new innovation number is assigned to that connection. If the connection has already been made in another network, the innovation number of the new connection will be the same as the innovation number of the network that already contains that connection. Crossovers are done only with connection genes, as node genes can be found using the connection genes. Consider the diagram below:</p>
<p align="center"><img src="https://user-images.githubusercontent.com/68528325/121792761-db693380-cbc6-11eb-97ab-cb4d11424a4e.png" alt="Crossover Diagram 1" width="400"/></p>
<p>The two sets of boxes represent the connection genes for each of the parents. The topmost number in each box represents the innovation number. Notice how connection 2→5 in Parent 1 has the same innovation number of 4 as connection 2→5 in Parent 2. The first step in crossovers are pairing the connection gene sets of both the parents by the innovation number of each gene:</p>
<p align="center"><img src="https://user-images.githubusercontent.com/68528325/121792808-61857a00-cbc7-11eb-85fa-4a40fd1183dd.png" alt="Crossover Diagram 2" width="400"/></p>
<p>In the diagram above, the shade of each box represents the weight of that connection and the boolean value “DISAB” represents whether or not the connection is disabled. The second step in a crossover is to determine the connection gene set of the offspring. For the genes that both parents have, the offspring will choose one of the genes randomly. For the connection genes that only one parent has, the offspring will inherit that parent’s connection gene. Disjoint genes are genes that do not have a matching parent gene and excess genes are disjoint genes at the end of the gene set. The reason that these are important is that if Parent 1 was said to be fitter (fitness refers to how well the individual has performed) then the excess genes of Parent 2 would be ignored and the offspring will not inherit them. Below is a diagram of what the genotype and phenotype of an offspring could look like after crossover:</p>
<p align="center"><img src="https://user-images.githubusercontent.com/68528325/121792830-9db8da80-cbc7-11eb-8812-79e5550d33ae.png" alt="Crossover Diagram 3" width="400"/></p>
<p>After the connection genes of the offspring have been determined, it is simple to identify the node gene set. This marks the end of the crossover stage of NEAT. Crossovers are used to progress species as a whole, however, there are two other stages NEAT uses to evolve the population: mutations and selections. Mutations essentially apply an operation to an individual to introduce noise in the way the NNs are structured. With mutations, NEAT can test out a completely different variation of NNs and see whether or not the individual performs well. Some operations include adding a connection or node at a random spot in the individual’s NN. Selection is the stage of NEAT in which genomes are chosen for later breeding. During selection, NEAT compares each genome’s NN structure and speciates them into separate groups. The genetic algorithm then rearranges the genomes in each species by their score. The score of a genome is how well the connections and nodes in the genome’s NN performed in the world. The next step in mutation is to kill a certain percentage of genomes in each species. For example, if the algorithm was to kill off 50% of the genomes, the lower performing half of the genomes of each species would be killed off. This process ensures that there will be variation in terms of NN topology in the next generation. By putting all these stages of NEAT together, the genetic algorithm can efficiently breed, evolve, and enhance the structure of neural networks until it has reached a structure that performs outstandingly well.</p>
